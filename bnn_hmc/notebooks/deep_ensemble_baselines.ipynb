{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from jax import numpy as jnp\n",
    "import numpy as onp\n",
    "import jax\n",
    "import tensorflow.compat.v2 as tf\n",
    "import argparse\n",
    "import time\n",
    "from collections import OrderedDict\n",
    "\n",
    "from bnn_hmc.utils import data_utils\n",
    "from bnn_hmc.utils import models\n",
    "from bnn_hmc.utils import losses\n",
    "from bnn_hmc.utils import checkpoint_utils\n",
    "from bnn_hmc.utils import cmd_args_utils\n",
    "from bnn_hmc.utils import logging_utils\n",
    "from bnn_hmc.utils import train_utils\n",
    "from bnn_hmc.utils import precision_utils\n",
    "from bnn_hmc.utils import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mean_std(arr):\n",
    "    arr = onp.asarray(arr)\n",
    "    return arr.mean(), arr.std()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IMDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
      "17465344/17464789 [==============================] - 1s 0us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<__array_function__ internals>:5: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "/home/pavel/anaconda3/envs/py38/lib/python3.8/site-packages/tensorflow/python/keras/datasets/imdb.py:159: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  x_train, y_train = np.array(xs[:idx]), np.array(labels[:idx])\n",
      "/home/pavel/anaconda3/envs/py38/lib/python3.8/site-packages/tensorflow/python/keras/datasets/imdb.py:160: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  x_test, y_test = np.array(xs[idx:]), np.array(labels[idx:])\n"
     ]
    }
   ],
   "source": [
    "dtype = jnp.float32\n",
    "train_set, test_set, task, data_info = data_utils.make_ds_pmap_fullbatch(\n",
    "    \"imdb\", dtype)\n",
    "\n",
    "net_apply, net_init = models.get_model(\"cnn_lstm\", data_info)\n",
    "net_apply = precision_utils.rewrite_high_precision(net_apply)\n",
    "\n",
    "labels = test_set[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "(_, predict_fn, _, _,_) = train_utils.get_task_specific_fns(task, data_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "need at least one array to stack",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-6d0ca4f2bbab>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mall_preds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0monp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_preds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mstack\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py38/lib/python3.8/site-packages/numpy/core/shape_base.py\u001b[0m in \u001b[0;36mstack\u001b[0;34m(arrays, axis, out)\u001b[0m\n\u001b[1;32m    421\u001b[0m     \u001b[0marrays\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0masanyarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0marr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    422\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 423\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'need at least one array to stack'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    424\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    425\u001b[0m     \u001b[0mshapes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0marr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: need at least one array to stack"
     ]
    }
   ],
   "source": [
    "all_preds = []\n",
    "for seed in range(12):\n",
    "    try:\n",
    "        checkpoint_dict = checkpoint_utils.load_checkpoint(\n",
    "            \"../runs/sgd/imdb/sgd_wd_3.0_stepsize_3e-07_batchsize_80_momentum_0.9_seed_{}\" \\\n",
    "            \"/model_step_499.pt\".format(seed))\n",
    "        _, params, net_state, _, _ = (\n",
    "            checkpoint_utils.parse_sgd_checkpoint_dict(checkpoint_dict))\n",
    "        predictions = onp.asarray(predict_fn(net_apply, params, net_state, test_set))\n",
    "        all_preds.append(predictions.copy())\n",
    "    except:\n",
    "        pass\n",
    "all_preds = onp.stack(all_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8294 +- 0.0058\n",
      "0.1299 +- 0.0112\n",
      "0.7554 +- 0.1450\n"
     ]
    }
   ],
   "source": [
    "accs = [metrics.accuracy(pred, labels) for pred in all_preds]\n",
    "nlls =  [metrics.nll(pred, labels) for pred in all_preds]\n",
    "eces =  [metrics.calibration_curve(pred, labels)[\"ece\"] for pred in all_preds]\n",
    "print(\"{:.4f} +- {:.4f}\".format(*get_mean_std(accs)))\n",
    "print(\"{:.4f} +- {:.4f}\".format(*get_mean_std(eces)))\n",
    "print(\"{:.4f} +- {:.4f}\".format(*get_mean_std(nlls)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.85608\n",
      "0.04337276110887527\n",
      "0.37756163\n"
     ]
    }
   ],
   "source": [
    "ens_preds = all_preds.mean(axis=0)\n",
    "print(metrics.accuracy(ens_preds, labels))\n",
    "print(metrics.calibration_curve(ens_preds, labels)[\"ece\"])\n",
    "print(metrics.nll(ens_preds, labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CIFAR-100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtype = jnp.float32\n",
    "train_set, test_set, task, data_info = data_utils.make_ds_pmap_fullbatch(\n",
    "    \"cifar100\", dtype)\n",
    "\n",
    "net_apply, net_init = models.get_model(\"resnet20_frn_swish\", data_info)\n",
    "net_apply = precision_utils.rewrite_high_precision(net_apply)\n",
    "\n",
    "labels = test_set[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "(_, predict_fn, _, _,_) = train_utils.get_task_specific_fns(task, data_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_preds = []\n",
    "for seed in range(12):\n",
    "    try:\n",
    "        checkpoint_dict = checkpoint_utils.load_checkpoint(\n",
    "            \"../runs/sgd/cifar100/sgd_wd_10.0_stepsize_1e-06_batchsize_80_momentum_0.9_seed_{}/model_step_499.pt\".format(seed))\n",
    "        _, params, net_state, _, _ = (\n",
    "            checkpoint_utils.parse_sgd_checkpoint_dict(checkpoint_dict))\n",
    "        predictions = onp.asarray(predict_fn(net_apply, params, net_state, test_set))\n",
    "        all_preds.append(predictions.copy())\n",
    "    except:\n",
    "        pass\n",
    "all_preds = onp.stack(all_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5004 +- 0.0130\n",
      "0.2068 +- 0.0107\n",
      "2.3171 +- 0.0924\n"
     ]
    }
   ],
   "source": [
    "accs = [metrics.accuracy(pred, labels) for pred in all_preds]\n",
    "nlls =  [metrics.nll(pred, labels) for pred in all_preds]\n",
    "eces =  [metrics.calibration_curve(pred, labels)[\"ece\"] for pred in all_preds]\n",
    "print(\"{:.4f} +- {:.4f}\".format(*get_mean_std(accs)))\n",
    "print(\"{:.4f} +- {:.4f}\".format(*get_mean_std(eces)))\n",
    "print(\"{:.4f} +- {:.4f}\".format(*get_mean_std(nlls)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6464\n",
      "0.117816821230948\n",
      "1.3727252\n"
     ]
    }
   ],
   "source": [
    "ens_preds = all_preds.mean(axis=0)\n",
    "print(metrics.accuracy(ens_preds, labels))\n",
    "print(metrics.calibration_curve(ens_preds, labels)[\"ece\"])\n",
    "print(metrics.nll(ens_preds, labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CIFAR-10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtype = jnp.float32\n",
    "train_set, test_set, task, data_info = data_utils.make_ds_pmap_fullbatch(\n",
    "    \"cifar10\", dtype)\n",
    "\n",
    "net_apply, net_init = models.get_model(\"resnet20_frn_swish\", data_info)\n",
    "net_apply = precision_utils.rewrite_high_precision(net_apply)\n",
    "\n",
    "labels = test_set[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "(_, predict_fn, _, _,_) = train_utils.get_task_specific_fns(task, data_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_preds = []\n",
    "for seed in range(12):\n",
    "    try:\n",
    "        checkpoint_dict = checkpoint_utils.load_checkpoint(\n",
    "            \"../runs/sgd/cifar10/sgd_wd_10.0_stepsize_3e-07_batchsize_80_momentum_0.9_seed_{}/model_step_499.pt\".format(seed))\n",
    "        _, params, net_state, _, _ = (\n",
    "            checkpoint_utils.parse_sgd_checkpoint_dict(checkpoint_dict))\n",
    "        predictions = onp.asarray(predict_fn(net_apply, params, net_state, test_set))\n",
    "        all_preds.append(predictions.copy())\n",
    "    except:\n",
    "        pass\n",
    "all_preds = onp.stack(all_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8548 +- 0.0049\n",
      "0.1002 +- 0.0061\n",
      "0.6743 +- 0.0446\n"
     ]
    }
   ],
   "source": [
    "accs = [metrics.accuracy(pred, labels) for pred in all_preds]\n",
    "nlls =  [metrics.nll(pred, labels) for pred in all_preds]\n",
    "eces =  [metrics.calibration_curve(pred, labels)[\"ece\"] for pred in all_preds]\n",
    "print(\"{:.4f} +- {:.4f}\".format(*get_mean_std(accs)))\n",
    "print(\"{:.4f} +- {:.4f}\".format(*get_mean_std(eces)))\n",
    "print(\"{:.4f} +- {:.4f}\".format(*get_mean_std(nlls)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.89919996\n",
      "0.018649099725484845\n",
      "0.31519425\n"
     ]
    }
   ],
   "source": [
    "ens_preds = all_preds.mean(axis=0)\n",
    "print(metrics.accuracy(ens_preds, labels))\n",
    "print(metrics.calibration_curve(ens_preds, labels)[\"ece\"])\n",
    "print(metrics.nll(ens_preds, labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py38",
   "language": "python",
   "name": "py38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
